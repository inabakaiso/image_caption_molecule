{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-07T07:59:35.227205Z","iopub.execute_input":"2021-06-07T07:59:35.227627Z","iopub.status.idle":"2021-06-07T08:18:14.127631Z","shell.execute_reply.started":"2021-06-07T07:59:35.227543Z","shell.execute_reply":"2021-06-07T08:18:14.126429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torchvision import models\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-06-07T08:18:14.129474Z","iopub.execute_input":"2021-06-07T08:18:14.129916Z","iopub.status.idle":"2021-06-07T08:18:15.63456Z","shell.execute_reply.started":"2021-06-07T08:18:14.129869Z","shell.execute_reply":"2021-06-07T08:18:15.633459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nfrom tqdm.auto import tqdm #重要\ntqdm.pandas()\n\ntrain = pd.read_csv('../input/bms-molecular-translation/train_labels.csv')\ntest = pd.read_csv('../input/bms-molecular-translation/sample_submission.csv')\n\ndef get_train_file_path(image_id):\n    return \"../input/bms-molecular-translation/train/{}/{}/{}/{}.png\".format(\n        image_id[0], image_id[1], image_id[2], image_id \n    )\n\ndef get_test_file_path(image_id):\n    return \"../input/bms-molecular-translation/test/{}/{}/{}/{}.png\".format(\n        image_id[0], image_id[1], image_id[2], image_id \n    )\n\ntrain['file_path'] = train['image_id'].progress_apply(get_train_file_path)\ntest['file_path'] = test['image_id'].progress_apply(get_test_file_path)\n\nprint(f'train.shape: {train.shape}  test.shape: {test.shape}')\ndisplay(train.head())\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-07T08:18:15.63684Z","iopub.execute_input":"2021-06-07T08:18:15.637155Z","iopub.status.idle":"2021-06-07T08:18:39.211547Z","shell.execute_reply.started":"2021-06-07T08:18:15.637123Z","shell.execute_reply":"2021-06-07T08:18:39.210177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nfor i in range(5):\n    img_path = train.loc[i, 'file_path']\n    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    img = img[:,:,np.newaxis]\n    label = train.loc[i, 'InChI']\n    print(img.shape)\n    plt.imshow(img)\n    plt.title(label)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T08:18:39.213792Z","iopub.execute_input":"2021-06-07T08:18:39.214252Z","iopub.status.idle":"2021-06-07T08:18:40.45105Z","shell.execute_reply.started":"2021-06-07T08:18:39.214205Z","shell.execute_reply":"2021-06-07T08:18:40.449925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scorring_function\nimport Levenshtein\ndef get_score(y_true, y_pred):\n    scores = list()\n    for true, pred in y_true, y_pred:\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-07T08:18:40.4538Z","iopub.execute_input":"2021-06-07T08:18:40.454238Z","iopub.status.idle":"2021-06-07T08:18:40.465528Z","shell.execute_reply.started":"2021-06-07T08:18:40.454189Z","shell.execute_reply":"2021-06-07T08:18:40.464349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef split_form(form):\n    string = ''\n    for i in re.findall(r\"[A-Z][^A-Z]*\", form):\n        elem = re.match(r\"\\D+\", i).group()\n        num = i.replace(elem, \"\")\n        if num == \"\":\n            string += f\"{elem} \"\n        else:\n            string += f\"{elem} {str(num)} \"\n    return string.rstrip(' ')\n\ndef split_form2(form):\n    string = ''\n    for i in re.findall(r\"[a-z][^a-z]*\", form):\n        elem = i[0]\n        num = i.replace(elem, \"\").replace('/', \"\")\n        num_string = ''\n        for j in re.findall(r\"[0-9]+[^0-9]*\", num):\n            num_list = list(re.findall(r'\\d+', j))\n            assert len(num_list) == 1, f\"len(num_list) != 1\"\n            _num = num_list[0]\n            if j == _num:\n                num_string += f\"{_num} \"\n            else:\n                extra = j.replace(_num, \"\")\n                num_string += f\"{_num} {' '.join(list(extra))} \"\n        string += f\"/{elem} {num_string}\"\n    return string.rstrip(' ')\n\ntrain['InChI_1'] = train['InChI'].progress_apply(lambda x: x.split('/')[1])\ntrain['InChI_text'] = train['InChI_1'].progress_apply(split_form) + ' ' + \\\n                            train['InChI'].apply(lambda x: '/'.join(x.split('/')[2:])).progress_apply(split_form2).values","metadata":{"execution":{"iopub.status.busy":"2021-06-07T08:18:40.468007Z","iopub.execute_input":"2021-06-07T08:18:40.469002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Tokenizer(object):\n    def __init__(self):\n        self.stoi = {}\n        self.itos = {}\n    \n    def __len__(self):\n        return len(self.stoi)\n    def fit_on_texts(self, texts):\n        vocab = set()\n        for text in texts:\n            vocab.update(text.split())\n        #copyしつつsort\n        vocab = sorted(vocab)\n        vocab.append('<sos>')\n        vocab.append('<eos>')\n        vocab.append('<pad>')\n        #i: index番号, s:value\n        for i, s in enumerate(vocab):\n            self.stoi[s] = i\n            #番号から語彙検索と語彙から番号検索を両方できるようにする\n        self.itos = {item[1]:item[0] for item in self.stoi.items()}\n    \n    def text_to_sequence(self, text):\n        sequence = []\n        sequence.append(self.stoi['<sos>'])\n        for s in text.split(' '):\n            sequence.append(self.stoi[s])\n        sequence.append(self.stoi['<eos>'])\n        return sequence\n    \n    def texts_to_sequences(self, texts):\n        sequences = []\n        for text in texts:\n            sequence = self.text_to_sequence(text)\n            sequences.append(sequence)\n        return sequences\n    \n    def sequence_to_text(self,sequence):\n        return ''.join(list(map(lambda x:self.itos[x], sequence)))\n    \n    def sequences_to_texts(self, sequences):\n        texts = []\n        for sequence in sequences:\n            text = self.sequence_to_text(sequence)\n            texts.append(text)\n        return texts\n    \n    def predict_caption(self, sequence):\n        caption = ''\n        for i in sequence:\n            if i == self.stoi['<sos>'] or i == self.stoi['<pad>']:\n                break\n            captions += self.itos[i]\n        return captions\n    \n    def prdict_captions(self, sequences):\n        captions = []\n        for sequence in sequences:\n            caption = self.predict_caption(sequence)\n            captions.append(caption)\n        return captions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train['InChI_text'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lengths = []\ntk0 = tqdm(train['InChI_text'].values, total=len(train))\nfor text in tk0:\n        seq = tokenizer.text_to_sequence(text)\n        length = len(seq) - 2\n        lengths.append(length)\ntrain['InChI_length'] = lengths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['InChI_length'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    debug = False\n    max_len = 275\n    print_freq = 1000\n    num_workers = 4\n    model_name = 'resnet'\n    size = 224\n    scheduler='CosineAnnealingLR' #学習率をepochによって変化\n    epochs = 35\n    T_max = 4\n    encoder_lr = 1e-4\n    decorder_lr = 4e-4\n    min_lr = 1e-6\n    batch_size = 64\n    weight_decay = 1e-6 #重み減衰 => over_fit対策　重みの値が大きくなることにペナルティをつける\n    gradient_acuumulation_steps = 1\n    max_grad_norm=5\n    attention_dim=256\n    embed_dim=256\n    decoder_dim=512\n    dropout=0.5\n    seed=42\n    n_fold=5\n    trn_fold=[0] # [0, 1, 2, 3, 4]\n    train=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.debug:\n    CFG.epochs = 1\n    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install timm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n\nimport os\nimport gc\nimport re\nimport math\nimport time\nimport random\nimport shutil\nimport pickle\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport Levenshtein\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose, Blur\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.determinstic = True\n\nseed_torch(CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = train.copy()\nFold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor i, (tr_idx, va_idx) in enumerate(Fold.split(folds, folds['InChI_length'])):\n    folds.loc[va_idx, 'fold'] = int(i) ##fold番号をインデックスのように指定\nfolds['fold'] = folds['fold'].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#*でリスト内を別途に取り出して処理することが可能\ndef image_transform(*,data):\n    if data =='train':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n                     ),\n            ToTensorV2(),\n        ])\n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n                     ),\n            ToTensorV2,\n        ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self,df, tokenizer, transform=None):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.file_paths = df['file_path'].values\n        self.labels = df['InChI_text'].values\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        file_path = self.file_paths[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        label = self.labels[idx]\n        label = self.tokenizer.text_to_sequence(label)\n        label_length = len(label) \n        #整数を扱う場合はlong型を使います（int型も別途ありますが、ニューラルネットのラベルとして受け付けてくれませんので、よほど使う機会はないです）\n        label_length = torch.LongTensor\n        return image, torch.LongTensor(label), label_length\n\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        self.file_paths = df['file_path']\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        image_path = self.file_paths\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=img)\n            image = augmented['image']\n        return image\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bms_collate(batch):\n    imgs, label, label_length = [],[],[]\n    for data_points in batch:\n        imgs.append(data_points[0])\n        labels.append(data_points[1])\n        label_length.append(data_points[2])\n    #Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise\n    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.stoi['<pad>'])\n    #torch.stack(imgs) => [], [], []... のような配置を[[],[],[],[],...]\n    return torch.stack(imgs), labels, torch.stack(label_length).reshape(-1, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size=CFG.size\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\ntrain_dataset = TrainDataset(train, tokenizer, transform=image_transform(data='train'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1):\n    image,label,label_length = train_dataset[i]\n    print(image.shape)\n    text = tokenizer.sequence_to_text(label.numpy())\n    plt.imshow(image.permute(1,2,0))\n    plt.show","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, model_name='resnet18', pretrained=False):\n        super().__init__()\n        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n        #最終層の入力.つまり最終層に入ってくるエッジの数\n        self.n_feaatures = self.cnn.fc.in_features\n        self.cnn.global_pool = nn.Identity()\n        self.cnn.fc = nn.Identity()\n    \n    def forward(self,x):\n        #画像データのサイズ (画像数, 色,　幅,　高さ)\n        bs = x.size(0)\n        #画像から特徴量抽出\n        features = self.cnn(x)\n        #(画像数, 高さ, 幅, 色) に変換している\n        features = features.permute(0,2,3,1)\n        return_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention,self).__init__()\n        #encoder,decoderの容器作り\n        self.encoder_att = nn.Linear(encoder_dim,decoder_dim) #(入力,出力) \n        self.decoder_att = nn.Linear(decoder_dim, attention_dim) # (batch_size, attention_dim)\n        self.full_att = nn.Linear(attention_dim, 1) #softmax\n        #使う活性化関数の定義\n        self.relu = nn.Relu()\n        self.softmax = nn.Softmax(dim=1) #dim = 出力数\n    \n    def forward(self, encoder_out, decoder_out):\n        #事故注意層はencoderのattention\n        att1 = self.encoder_att(encoder_out) # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_out) # (batch_size, attention_dim)\n        #次元があっていないと足し算できないのでunsqueeze\n        #torch.squeezeは要素数が1のみの軸を削除\n        #num_pixel = (高さ,　幅)\n        att = self.full_att(self.relu(att1+att2.unsqueeze(1))).squeeze(2) #(batch_size, (height * width))\n        \n        alpha = self.softmax(att) # (batch_size, (height,width))\n        attention_weighted_encoding =(encoder_out * alpha.squeeze(2))\n        \n        return attention_weight_encoding, alpha\n    \n\nclass DecoderWithAttention(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n        super(DecoderWithAttention, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = decoder_dim\n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        self.device = device\n        self.attention = Attention(encoder_dim,decoder_dim, attention_dim)\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = dropout\n        self.devce = device\n        #decoding_cell\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True) #encoderの出力特徴量と入力特徴量をconcatするので\n        #LSTMのセルと隠れ層の定義\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        #sigmoid_acctivate_gate\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        self.sigmoid = nn.sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)\n    \n    def load_pretrained_embeddings(self, fine_tune=True):\n        for p in self.embedding.parameters():\n            #requires_grad=trueで更新される\n            p.requires_grad = fine_tune\n    \n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n    \n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = n.Parameter(embeddings)\n        \n    def fine_tune_embeddings(self, fine_tune=True):\n        for p in self.embedding.parameters():\n            p.requires_grad = True\n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1) #num_pixels\n        #hidden層の計算encoderの出力を入力として計算している\n        h = self.init_h(mean_encoder_out) # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n    \n    def forward(self, encoder_out, encoded_captions, caption_length):\n        batch_size = encoder_out(0) #0次元が画像枚数\n        encoder_dim = encoder_out(-1)\n        vocab_size = self.vocab_size\n        #テンソルの形状を再形成する.　形状の総数が一致していないとerror\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim) # (batch_size, num_pixel, encoder_dim)\n        num_pixels = encoder_out(1)\n        caption_length, sort_idx = caption_length.squeeze(1).sort(dim=0, descending=True)\n        #labelをデータの出力されたものに合わせる\n        encoder_out = encoder_out[sort_idx]\n        encoded_captions = encoded_captions[sort_idx]\n        embeddings  = self.embedding(encoded_captions)\n        #encoded_features => LSTMCell\n        h,c = self.init_hidden_state(encoder_out)\n        decode_lengths = (caption_length - 1).tolist()\n        predictions = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n        #predict sequence\n        for t in range(max(decode_length)):\n            #padding\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size])) ## gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weight_encoding\n            #cat => 第三引数は出力時のテンソルのサイズを変更している\n            h, c = self.decode_step(torch.cat([embeddings, attention_weight], dim=1), (h[:batch_size_t],c[batch_size_t]))\n            preds = self.fc(self.fc(self.dropout(h)))\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n        return predictions, encoded_captions, decode_length, alphas, sort_ind\n    \n    def predict(self, encoder_out, decoder_length, tokenizer):\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        start_tokens = np.ones(batch_size, dtype=torch.long).to(self.device)\n        #最初の画像はないのでstart tokensで代用\n        embeddings = self.embedding(start_tokens)\n        #最初のLSTMの出力層,最初はencoderの出力を平均するだけ\n        h, c = self.init_hidden_state(encoder_out) #(batch_size, encoder_dim)\n        for t in range(decoder_length):\n            #attention\n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            #横軸の直接渡す法のhの処理\n            gate = self.sigmoid(self.f_beta(h))\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(torch.cat([embeddings, attention_weighted_encoded], dim=1), (h,c))\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n                break\n            embeddings = self.embedding(torch.argmax(preds, -1))\n        return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Helper functions\nclass AverageMator(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum /self.count\n    \ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%m %ds' % (m, s)\n    \ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es -s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n    \ndef train_fn(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epochs,\n            encode_scheduler, decoder_scheduler, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    encoder.train()\n    decoder.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels, label_length) in enumerate(train_loader):\n        #measure_loading_time\n        data_time.upgrade(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        batch_size = image.size(0)\n        features = encoder(images)\n        predictions, caps_sorted, decode_lengths, alpha, sort_idx = decoder(features, labels, label_length)\n        targets = caps_sorted[:,1:]\n        #batchでパックしながらpadding\n        predictions = pack_padded_sequence(predictions, decode_length, batch_first=True).data\n        targets = pack_padded_sequence(targets, decode_length, batch_first=True)\n        loss = criterion(predictions, targets)\n        losses.update(loss.item(), batch_size)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        loss.backward()\n        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            global_step += 1\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n                  #'Encoder LR: {encoder_lr:.6f}  '\n                  #'Decoder LR: {decoder_lr:.6f}  '\n                  .format(\n                   epoch+1, step, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)/len(train_loader)),\n                   encoder_grad_norm=encoder_grad_norm,\n                   decoder_grad_norm=decoder_grad_norm,\n                   #encoder_lr=encoder_scheduler.get_lr()[0],\n                   #decoder_lr=decoder_scheduler.get_lr()[0],\n                   ))\n    return losses.avg\n\n\ndef valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    # switch to evaluation mode\n    encoder.eval()\n    decoder.eval()\n    text_preds = []\n    start = end = time.time()\n    for step, (images) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        batch_size = images.size(0)\n        with torch.no_grad():\n            features = encoder(images)\n            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n        _text_preds = tokenizer.predict_captions(predicted_sequence)\n        text_preds.append(_text_preds)\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  .format(\n                   step, len(valid_loader), batch_time=batch_time,\n                   data_time=data_time,\n                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n                   ))\n    text_preds = np.concatenate(text_preds)\n    return text_preds\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_logger():\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    return logger\n\nLOGGER = init_logger()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(folds, fold):\n    \n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    \n    # loader\n    # ====================================================\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds['InChI'].values\n\n    train_dataset = TrainDataset(train_folds, tokenizer, transform=image_transform(data='train'))\n    valid_dataset = TestDataset(valid_folds, transform=image_transform(data='valid'))\n\n    train_loader = DataLoader(train_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=True, \n                              num_workers=CFG.num_workers, \n                              pin_memory=True,\n                              drop_last=True, \n                              collate_fn=bms_collate)\n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=False, \n                              num_workers=CFG.num_workers,\n                              pin_memory=True, \n                              drop_last=False)\ndef get_scheduler(optimizer):\n    if CFG.scheduler=='ReduceLROnPlateau':\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n    elif CFG.scheduler=='CosineAnnealingLR':\n        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n    return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    encoder = Encoder(CFG.model_name, pretrained=True)\n    encoder.to(device)\n    encoder_optimizer = Adam(encoder.parameters(), lr=CFG.encoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    encoder_scheduler = get_scheduler(encoder_optimizer)\n    \n    decoder = DecoderWithAttention(attention_dim=CFG.attention_dim,\n                                   embed_dim=CFG.embed_dim,\n                                   decoder_dim=CFG.decoder_dim,\n                                   vocab_size=len(tokenizer),\n                                   dropout=CFG.dropout,\n                                   device=device)\n    decoder.to(device)\n    decoder_optimizer = Adam(decoder.parameters(), lr=CFG.decoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    decoder_scheduler = get_scheduler(decoder_optimizer)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.stoi[\"<pad>\"])\n\n    best_score = np.inf\n    best_loss = np.inf\n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n                            encoder_optimizer, decoder_optimizer, epoch, \n                            encoder_scheduler, decoder_scheduler, device)\n\n        # eval\n        text_preds = valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device)\n        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n        LOGGER.info(f\"preds: {text_preds[:5]}\")\n        \n        # scoring\n        score = get_score(valid_labels, text_preds)\n        \n        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n            encoder_scheduler.step(score)\n        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n            encoder_scheduler.step()\n        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n            encoder_scheduler.step()\n            \n        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n            decoder_scheduler.step(score)\n        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n            decoder_scheduler.step()\n        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n            decoder_scheduler.step()\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        \n        if score < best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'encoder': encoder.state_dict(), \n                        'encoder_optimizer': encoder_optimizer.state_dict(), \n                        'encoder_scheduler': encoder_scheduler.state_dict(), \n                        'decoder': decoder.state_dict(), \n                        'decoder_optimizer': decoder_optimizer.state_dict(), \n                        'decoder_scheduler': decoder_scheduler.state_dict(), \n                        'text_preds': text_preds,\n                       },\n                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n\n    \"\"\"\n    Prepare: 1.train  2.folds\n    \"\"\"\n\n    if CFG.train:\n        # train\n        oof_df = pd.DataFrame()\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                train_loop(folds, fold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}